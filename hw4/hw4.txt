Homework 4
High Performance Computing
Kate Storey-Fisher
2021-04-21


1. Greene Network Test

I use the pingpong.cpp program to test the bandwidth and 
latency on Greene. I use two CPUs on each of two nodes
(4 tasks total), and obtain the result:

proc0=0, proc1=3, Rank 2/4 running on cs062.nyu.cluster.
proc0=0, proc1=3, Rank 0/4 running on cs031.nyu.cluster.
proc0=0, proc1=3, Rank 1/4 running on cs031.nyu.cluster.
proc0=0, proc1=3, Rank 3/4 running on cs062.nyu.cluster.
pingpong latency: 5.351603e-03 ms
pingpong bandwidth: 1.213463e+01 GB/s

We can see that the procs we are pingponging between,
0 and 3, are on different nodes (0 on cs031, 3 on cs062).
We still get quite a small latency; this is because Greene
has such a good network. We also get reasonably high band-
width.

I compare this to pingponging between two processes on the 
same node, 0 and 1. I obtain the results:

proc0=0, proc1=1, Rank 0/4 running on cs031.nyu.cluster.
proc0=0, proc1=1, Rank 2/4 running on cs062.nyu.cluster.
proc0=0, proc1=1, Rank 1/4 running on cs031.nyu.cluster.
proc0=0, proc1=1, Rank 3/4 running on cs062.nyu.cluster.
pingpong latency: 3.560120e-04 ms
pingpong bandwidth: 2.444405e+01 GB/s

We can see that the latency is faster by an order of mag-
nitude, and the bandwidth is larger by a factor of two.
This makes sense because the information no longer has to 
go through the network, but rather just be passed within
the same node.


2. MPI Ring Communication

I implement a program to send an integer around in a ring,
across nodes. As an initial test, I run 4 processes on 4
different nodes, for 2 loops. Each process adds its rank to
the integer. The output for this is below:

Rank 0/4 running on cs032.nyu.cluster.
Rank 1/4 running on cs064.nyu.cluster.
Rank 3/4 running on cs151.nyu.cluster.
Rank 2/4 running on cs096.nyu.cluster.
Loop 0/2
Loop 0: Proc 0 just added to msg, now msg=0. Sending to proc 1
Loop 0: Proc 1 just received msg (msg=0)
Loop 0: Proc 1 just added to msg, now msg=1. Sending to proc 2
Loop 0: Proc 3 just received msg (msg=3)
Loop 0: Proc 3 just added to msg, now msg=6. Sending to proc 0
Loop 0: Proc 2 just received msg (msg=1)
Loop 0: Proc 2 just added to msg, now msg=3. Sending to proc 3
Loop 0: Proc 0 just received msg (msg=6)
Loop 1/2
Loop 1: Proc 0 just added to msg, now msg=6. Sending to proc 1
Loop 1: Proc 0 just received msg (msg=12)
Loop 1: Proc 3 just received msg (msg=9)
Loop 1: Proc 3 just added to msg, now msg=12. Sending to proc 0
Loop 1: Proc 2 just received msg (msg=7)
Loop 1: Proc 2 just added to msg, now msg=9. Sending to proc 3
Loop 1: Proc 1 just received msg (msg=6)
Loop 1: Proc 1 just added to msg, now msg=7. Sending to proc 2
final message: 12
expected message: 12

The loop values print out of order, but they are communicating
around the ring in the correct order, as we can tell from the 
value of the message (msg).

We now run this with a larger N, N=1000000, in order to estimate
the latency, and find:

final message: 6000000
expected message: 6000000
ring latency: 1.738967e-03 ms

This is quite a high latency, confirming our pingpong measurment
that Greene has a very fast network.

Finally, we send around a large array, of size ~2MB. For 10000 
loops, we obtain the following latentcy and bandwidth:

Passing array of 500000 ints (2.000000 MB) around network loop
ring latency: 1.563896e-01 ms
ring bandwidth: 1.278857e+01 GB/s

The latency is larger, but now we are interested in the bandwidth,
which is once again quite good on Greene.
