Homework 4
High Performance Computing
Kate Storey-Fisher
2021-04-21


1. Greene Network Test

I use the pingpong.cpp program to test the bandwidth and 
latency on Greene. I use two CPUs on each of two nodes
(4 tasks total), and obtain the result:

proc0=0, proc1=3, Rank 2/4 running on cs062.nyu.cluster.
proc0=0, proc1=3, Rank 0/4 running on cs031.nyu.cluster.
proc0=0, proc1=3, Rank 1/4 running on cs031.nyu.cluster.
proc0=0, proc1=3, Rank 3/4 running on cs062.nyu.cluster.
pingpong latency: 5.351603e-03 ms
pingpong bandwidth: 1.213463e+01 GB/s

We can see that the procs we are pingponging between,
0 and 3, are on different nodes (0 on cs031, 3 on cs062).
We still get quite a small latency; this is because Greene
has such a good network. We also get reasonably high band-
width.

I compare this to pingponging between two processes on the 
same node, 0 and 1. I obtain the results:

proc0=0, proc1=1, Rank 0/4 running on cs031.nyu.cluster.
proc0=0, proc1=1, Rank 2/4 running on cs062.nyu.cluster.
proc0=0, proc1=1, Rank 1/4 running on cs031.nyu.cluster.
proc0=0, proc1=1, Rank 3/4 running on cs062.nyu.cluster.
pingpong latency: 3.560120e-04 ms
pingpong bandwidth: 2.444405e+01 GB/s

We can see that the latency is faster by an order of mag-
nitude, and the bandwidth is larger by a factor of two.
This makes sense because the information no longer has to 
go through the network, but rather just be passed within
the same node.


2. MPI Ring Communication

I implement a program to send an integer around in a ring,
across nodes. As an initial test, I run 4 processes on 4
different nodes, for 2 loops. Each process adds its rank to
the integer. The output for this is below:

Rank 0/4 running on cs032.nyu.cluster.
Rank 1/4 running on cs064.nyu.cluster.
Rank 3/4 running on cs151.nyu.cluster.
Rank 2/4 running on cs096.nyu.cluster.
Loop 0/2
Loop 0: Proc 0 just added to msg, now msg=0. Sending to proc 1
Loop 0: Proc 1 just received msg (msg=0)
Loop 0: Proc 1 just added to msg, now msg=1. Sending to proc 2
Loop 0: Proc 3 just received msg (msg=3)
Loop 0: Proc 3 just added to msg, now msg=6. Sending to proc 0
Loop 0: Proc 2 just received msg (msg=1)
Loop 0: Proc 2 just added to msg, now msg=3. Sending to proc 3
Loop 0: Proc 0 just received msg (msg=6)
Loop 1/2
Loop 1: Proc 0 just added to msg, now msg=6. Sending to proc 1
Loop 1: Proc 0 just received msg (msg=12)
Loop 1: Proc 3 just received msg (msg=9)
Loop 1: Proc 3 just added to msg, now msg=12. Sending to proc 0
Loop 1: Proc 2 just received msg (msg=7)
Loop 1: Proc 2 just added to msg, now msg=9. Sending to proc 3
Loop 1: Proc 1 just received msg (msg=6)
Loop 1: Proc 1 just added to msg, now msg=7. Sending to proc 2
final message: 12
expected message: 12

The loop values print out of order, but they are communicating
around the ring in the correct order, as we can tell from the 
value of the message (msg).

We now run this with a larger N, N=1000000, in order to estimate
the latency, and find:

final message: 6000000
expected message: 6000000
ring latency: 1.738967e-03 ms

This is quite a high latency, confirming our pingpong measurment
that Greene has a very fast network.

Finally, we send around a large array, of size ~2MB. For 10000 
loops, we obtain the following latentcy and bandwidth:

Passing array of 500000 ints (2.000000 MB) around network loop
ring latency: 1.563896e-01 ms
ring bandwidth: 1.278857e+01 GB/s

The latency is larger, but now we are interested in the bandwidth,
which is once again quite good on Greene.


3. (a) MPI Scan 

I have implemented the scan function (prefix sum) in parallel 
with MPI. I use the MPI_Scatter function to distribute the 
initial array to all of the processes, the MPI_Allgather 
function to collect the offsets for each chunk, and finally 
the MPI_Gather function to merge the computed chunks back
into the full result array.

I compare this to a serial scan, just computed by the
first process. For all the timings below, I achieve zero
error, meaning my parallel version is computing the answer
exactly correctly.

The timing of my parallel implementation depends on the number
of nodes and processes, but it is often slower than the 
serial version. This may be due to high latency and low 
bandwidth compared to the computation required. 

For N = 10000000:
nodes=1, processes=10
sequential-scan = 0.042699s
parallel-scan   = 0.082480s
nodes=1, processes=20
sequential-scan = 0.042764s
parallel-scan   = 0.098069s
nodes=2, processes=10 (5 per node)
sequential-scan = 0.049287s
parallel-scan   = 0.245692s
nodes=2, processes=20 (10 per node)
sequential-scan = 0.049196s
parallel-scan   = 0.501006s

We can see that we get worse performance when using 
more than one node. But even on one node, the time is 
slower; this may be because MPI requires a lot of 
overhead, and for more compute-heavy tasks the parallelization
would greatly decrease the time needed.


4. Final project proposal

For my Physics PhD research, I have been working on a new 
method for estimating the two-point correlation function (a 
summary statistic of point processes that is important in 
cosmology). I have implemented it at https://github.com/kstoreyf/suave, 
as a fork of an existing state-of-the-art package Corrfunc
(https://github.com/manodeep/Corrfunc). Corrfunc is highly 
parallelized, with OpenMP and vector intrinsics, using a 
sophisticated mesh structure. My extension currently utilizes 
the OpenMP part but is not vectorized. For my final project,
I plan to add vectorization to my kernels.

In more detail, Corrfunc computes the standard two-point 
correlation function (2pcf) on a set of points (locations), 
which is the excess probability of a pair being located a 
certain distance r apart, compared to a random Poisson 
distribution. The estimation of this boils down to computing 
the distances of N^2 pairs and building up a histogram of 
these separations. Corrfunc speeds this up by dividing the 
volume into a grid to minimize the required number of distance 
calculations, among many other optimizations.

My code, suave, implements a new estimator for the 2pcf, which 
uses the pair separations to compute a continuous 2pcf. In 
practice, this requires, at every pair separation r, evaluating 
K functions f_k(r), and adding the results to a results vector 
of length K. (For example, if f_k(r) is a tophat function, this
reduces to the standard, where K is then the number of bins.) 
For my research I have forked Corrfunc, and in the deep kernels 
where it finally computes the separation, added a call to f_k(r).
This function could be standard functions like splines (which 
then produces essentially a spline fit to the data) or trig 
functions, or a general user-defined function on some fine grid, 
which requires interpolation. For my final project, I will 
vectorize these functions f_k(r). This may also require 
propogation in other parts of the code. Specifically, I will 
implement AVX intrinsics, and if time allows, also SSE intrinsics.
