Homework 2
Kate Storey-Fisher
2021-03-06

1. Finding Memory Bugs

I have used valgrind to find the bugs in the val_test programs.
Here are the bugs I have found and the solutions:

val_test01.cpp
Line 80: "for ( i = 2; i <= n; i++ )"
This loop includes the value n, which is one past the final malloced
address in the array. I fixed "<= n" to "< n", so we don't try to access
un-alloacted memory.

val_test02.cpp
Line 81:
Only the first 5 values of the length-10 array are initialized. 
The others are left uninitalialized.
I solve this by initializing the latter 5 values to zero.


2. Optiminizing matrix-matrix multiplication

I have improved the MM multiplication code using blocking.
I am running on a Intel(R) Xeon(R) Platinum 8268 CPU @ 2.90GHz machine 
with 24 CPU cores and cache size 36608 KB = 36.608 MB.

The reference solution has the following timings for a variety
of matrix dimensions (assuming a flop rate 2*m*n + m*k*(n+1), 
as the matrix is in column-major order but we are looping over it
in the order N->K->M):
 Dimension       Time    Gflop/s       GB/s        Error
        16   0.130744  15.297049   9.082623 0.000000e+00
       256   0.120594  16.694624   8.445132 0.000000e+00
       496   0.343425   6.395664   3.217174 0.000000e+00
       736   0.383487   6.237845   3.131635 0.000000e+00
       976   0.596708   6.232294   3.125725 0.000000e+00
      1216   0.618071   5.818253   2.916303 0.000000e+00
      1456   1.236381   4.993004   2.501646 0.000000e+00
      1696   2.171038   4.494073   2.251011 0.000000e+00
      1936   3.583857   4.049444   2.027860 0.000000e+00

We first investigate the importance of the ordering of the loops.
We find that ordering the loops n->k->m (j->p->i) or k->n->m (p->j->i) is faster
than any other ordering by a factor of ~4 (these two are similarly fast).
This is related to the fact that the matrices are stored in column-major
order. For the n->k->m case, we can read in each column j of B in the outer 
loop and keep it in cache while we loop over the rows of A, leading to only
nk reads from B. We still must perform nkm reads from A, but due to the loop
order, we can do these in only kn memory transactions, as for each of the k
columns we can loop over the m rows. The reading and writing from C each takes
mn operations, for a total of mops=2mn + nk(m+1). The other loop orderings 
require more reads, and/or more memory transactions. For instance, the order
n->m->k requires looping through all of the m values in order to read all of
the kn columns of B, requiring mnk reads (although if column j of B stays in
the cache when looping through the m rows of A, we would expect nk reads; the
slowdown we see may mean the cache is not large enough for this. The flops 
remain the same for all of these orderings, flops=2nmk.

We then implement a one-level blocking scheme. The flop calculation is the 
same as for non-blocked: 2mnk. The memory calculation now must take into
account that we are loading blocks into memory rather than rows/columns.
Assume a blocksize B, with N blocks for the dimension measured by n, M blocks
for m, and K blocks for K. Then we read in C blocks of size B^2 MN times, and 
write it the same amount. We read in A blocks of MNK times, and same with B. 
This totals 2B^2*MN(K+1) = 2mn(k/B + 1). We see that for large k, computational
intensity q is flops/memory = 2mnk/(2mn(k/B + 1))=~B.

We find the following timings for the blocked implementation, with various
BLOCK_SIZEs:
BLOCK_SIZE = 4:
Dimension       Time    Gflop/s       GB/s        Error
         4   0.543334   3.680976   1.840488 0.000000e+00
       252   0.441362   4.568533   1.160262 0.000000e+00
       500   0.503166   4.471689   1.126866 0.000000e+00
       748   0.571469   4.394036   1.104383 0.000000e+00
       996   0.890862   4.436368   1.113546 0.000000e+00
      1244   0.867408   4.438822   1.113274 0.000000e+00
      1492   1.500954   4.425568   1.109358 0.000000e+00
      1740   2.382795   4.421718   1.107971 0.000000e+00
      1988   3.562507   4.410862   1.104934 0.000000e+00
BLOCK_SIZE = 8:
 Dimension       Time    Gflop/s       GB/s        Error
         8   0.411819   4.856505   1.214126 0.000000e+00
       256   0.513162   3.923254   0.505732 0.000000e+00
       504   0.449504   4.556992   0.578666 0.000000e+00
       752   0.552321   4.619690   0.583604 0.000000e+00
      1000   0.863414   4.632774   0.583730 0.000000e+00
      1248   0.845773   4.596422   0.578236 0.000000e+00
      1496   1.436834   4.660345   0.585658 0.000000e+00
      1744   2.289649   4.633407   0.581833 0.000000e+00
      1992   3.422203   4.619470   0.579753 0.000000e+00
BLOCK_SIZE = 16:
 Dimension       Time    Gflop/s       GB/s        Error
        16   0.189501  10.554074   1.319259 0.000000e+00
       256   0.244081   8.248352   0.547742 0.000000e+00
       496   0.215763  10.179830   0.656763 0.000000e+00
       736   0.241309   9.913136   0.633040 0.000000e+00
       976   0.369216  10.072310   0.639839 0.000000e+00
      1216   0.367817   9.776854   0.619094 0.000000e+00
      1456   0.609601  10.126714   0.639875 0.000000e+00
      1696   0.980482   9.951027   0.627807 0.000000e+00
      1936   1.445361  10.040835   0.632739 0.000000e+00
BLOCK_SIZE = 32:
 Dimension       Time    Gflop/s       GB/s        Error
        32   0.168949  11.838047   0.739878 0.000000e+00
       256   0.211449   9.521302   0.334733 0.000000e+00
       480   0.191440  11.553720   0.385124 0.000000e+00
       704   0.185548  11.282689   0.368611 0.000000e+00
       928   0.283493  11.276180   0.364532 0.000000e+00
      1152   0.295416  10.350298   0.332431 0.000000e+00
      1376   0.503803  10.342485   0.330719 0.000000e+00
      1600   0.780725  10.492812   0.334458 0.000000e+00
      1824   1.139093  10.654802   0.338804 0.000000e+00
BLOCK_SIZE = 64:
 Dimension       Time    Gflop/s       GB/s        Error
        64   0.144761  13.816955   0.431780 0.000000e+00
       256   0.178563  11.274827   0.220211 0.000000e+00
       448   0.180676  11.943881   0.213284 0.000000e+00
       640   0.188084  11.150071   0.191642 0.000000e+00
       832   0.183414  12.560232   0.211350 0.000000e+00
      1024   0.215178   9.980017   0.165684 0.000000e+00
      1216   0.289229  12.433378   0.204496 0.000000e+00
      1408   0.499248  11.182050   0.182661 0.000000e+00
      1600   0.657034  12.468149   0.202607 0.000000e+00
      1792   1.074968  10.706528   0.173264 0.000000e+00
      1984   1.280944  12.193404   0.196668 0.000000e+00
BLOCK_SIZE = 128:
 Dimension       Time    Gflop/s       GB/s        Error
       128   0.205303   9.745020   0.152266 0.000000e+00
       256   0.208307   9.664891   0.113260 0.000000e+00
       384   0.210232   9.696088   0.101001 0.000000e+00
       512   0.220959   9.718937   0.094911 0.000000e+00
       640   0.216771   9.674497   0.090698 0.000000e+00
       768   0.281815   9.644296   0.087904 0.000000e+00
       896   0.298710   9.632392   0.086003 0.000000e+00
      1024   0.222077   9.669984   0.084990 0.000000e+00
      1152   0.318060   9.613430   0.083450 0.000000e+00
      1280   0.435568   9.629500   0.082754 0.000000e+00
      1408   0.581382   9.602327   0.081838 0.000000e+00
      1536   0.753579   9.617777   0.081400 0.000000e+00
      1664   0.959573   9.603114   0.080795 0.000000e+00
      1792   1.202084   9.574345   0.080142 0.000000e+00
      1920   1.486311   9.524101   0.079368 0.000000e+00

We see that the blocking gives us significant speedup once we get up to
BLOCK_SIZE=32. The optimal value is BLOCK_SIZE=64, which is slighly faster
than 32; it is faster than the reference time by a factor of about 2.
The performance starts to degrade slightly once we get to
BLOCK_SIZE=128, though it is still an improvement.

The peak flop rate for my machine can be computed from the clock rate,
2.90GHz. Due to vectorization we assume it can can do 4 computations/cycle
(this is an estimate based on typical computations/cycle; my machine
does not report this). This gives a max theoretical flop rate of 
11.6Gflop/s. My optimal blocked version achieves flop rates up to 
12.5Gflop/s, slightly above the peak rate. We should not be able to achieve
this high a flop rate compared to peak, so it is unclear what is going on.
The Greene nodes may use Turbo which gives 3.90GHz, which makes a bit more 
sense. Other Greene fancy speedups may help explain this difference..


3. Approximating Special Functions Using Taylor Series & Vectorization

I have improved the function sin4_intrin() for AVX intrinsics to an
accuracy of about 12 digits, by adding terms to the Taylor series up
to x^11, using the AVX summation and multiplication functions
_mm256_mul_pd() and _mm256_add_pd().

Extra credit: I have implemented an approximation for sinx of any x,
not just in the interval [-pi/4, pi/4]. To do this I use Euler's formula.
For an angle t+pi/2, we have e^(i*(t+pi/2)) = ie^(it). Expanding out both
sides, we get cos(t+pi/2) + isin(t+pi/2) = icost - sint. Equating the complex
parts, we obtain sin(t+pi/2) = cost. Thus to evaluate sinx for x that is from
pi/4 to 3pi/4, we can subtract pi/2 and evaluate cost. Similarly, we find that
for x from -3pi/4 to -pi/4, we can use sin(t-pi/2) = -cost, and for x from
3pi/4 to pi and -pi to -3pi/4, we can use sin(t+pi)=-sin(t). For values 
outside the range -pi to pi, we can normalize to this range before evaluating. 

I have implemented this in sin4_taylor_extended, and for all x it achieves a 
very small error compared to the reference solution. I use a Taylor approximation
for cosx up to x^10 order. This does require some overhead to figure out which
case we are in. My implementation is very inefficient and takes a long time
(10x reference solution), but it works.

I also implement this method in sinx_intrin_extended. I achieve a similarly small 
error. Here we can't decide for each individual x which case to compute because
of the blocking of 4 xs for vectorization, so for each 4 I compute both the sinx
and cosx, and then afterwards decide which to save to the results based on the value
of x. Again I am sure there is a much more efficient way, but this does work.
