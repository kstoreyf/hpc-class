Homework 2
High Performance Computing
Kate Storey-Fisher
2021-03-27


1. OpenMP warm-up
a) For the first for-loop parallel region, the first thread takes sum_1^(n/2-1)[i] ms, 
and the second thread takes sum_n/2^(n-1)[i] ms. For the second for-loop, these timings
are swapped, as the time is f(n-1). There is an implicit barrier between the parallel
regions, so in the first region, loop 1 spends sum_n/2^(n-1)[i] - sum_i^(n/2-1)[i] =
n^2/4 ms. The same goes for loop 2 in the second region, for a total of n^2/2 ms of 
waiting. Here we have assumed that n is odd, but it would be close to this for even n,
especially as it gets large.
b) If we instead use schedule(static,1), the threads will no longer take the first vs
second half, but every other value of i. This will even out the time, as the time spent
in f scales linearly with i. If n is odd, the timing should be exactly the same; if it
is even there will be slightly differences.
c) If we use schedule(dynamic,1), the threads will be given values of i in no particular
order. This will cause slight unevenness between the threads, but for large N, the times
should end up being pretty even. So no, the timing will not improve, but it will only
worsen slightly.
d) We could add "nowait" to the end of the first "#pragma omp for schedule(static)"
clause, so that the threads would not wait for each other after the first loop. Because
their workload switches in the second loop, this would overall even out the time both
spend performing the work in function f.
