Homework 2
High Performance Computing
Kate Storey-Fisher
2021-03-27


1. OpenMP warm-up
a) For the first for-loop parallel region, the first thread takes sum_1^(n/2-1)[i] ms, 
and the second thread takes sum_n/2^(n-1)[i] ms. For the second for-loop, these timings
are swapped, as the time is f(n-1). There is an implicit barrier between the parallel
regions, so in the first region, loop 1 spends sum_n/2^(n-1)[i] - sum_i^(n/2-1)[i] =
n^2/4 ms. The same goes for loop 2 in the second region, for a total of n^2/2 ms of 
waiting. Here we have assumed that n is odd, but it would be close to this for even n,
especially as it gets large.
b) If we instead use schedule(static,1), the threads will no longer take the first vs
second half, but every other value of i. This will even out the time, as the time spent
in f scales linearly with i. If n is odd, the timing should be exactly the same; if it
is even there will be slightly differences.
c) If we use schedule(dynamic,1), the threads will be given values of i in no particular
order. This will cause slight unevenness between the threads, but for large N, the times
should end up being pretty even. So no, the timing will not improve, but it will only
worsen slightly.
d) We could add "nowait" to the end of the first "#pragma omp for schedule(static)"
clause, so that the threads would not wait for each other after the first loop. Because
their workload switches in the second loop, this would overall even out the time both
spend performing the work in function f.

2. Finding OpenMP bugs
The solutions to these bugs are contained in the files omp_solved{2,3,4,5,6}.c.

3. Parallel Scan in OpenMP
I have parallelized the scan operation in omp-scan.cpp. I am running it on an Intel(R) 
Xeon(R) Platinum 8268 CPU @ 2.90GHz machine with 24 CPU cores. The timings are as follows:
num_threads     time_serial     time_parallel
1               0.395430s       0.464113s
2               0.398467s       0.251680s
4               0.396774s       0.133399s
8               0.405978s       0.077052s
16              0.396201s       0.056059s
24              0.393506s       0.070141s
32              0.400545s       0.068314s
64              0.419367s       0.057954s
We can see that we get a speedup of not quite a factor of the number of threads, but still
substantial. The speedup in parallel plateaus once we hit ~16 threads. This is a bit odd 
as we have 24 cores, but perhaps the overhead of thread splitting gets high with so
many threads.

4. OpenMP version of 2D Jacobi/Gauss-Seidel smoothing
I have implemented serial and parallel versions of these algorithms. For f(x,y)=1 and 
a maximum of 1000 iterations, we get the following timings for Jacobi:

